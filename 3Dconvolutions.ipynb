{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3Dconvolutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrMe5Pl8Y85r8p5aJZJbdZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filsto/3DNN/blob/main/3Dconvolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMbB3n0Zf_i2"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense\n",
        "from keras.layers import Dropout, Input, BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adadelta\n",
        "import plotly.graph_objs as go\n",
        "from matplotlib.pyplot import cm\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "import keras\n",
        "import h5py\n",
        "\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.file('../input/full_dataset_vectors.h5', 'r') as dataset:\n",
        "  x_train = dataset[\"X_train\"][:]\n",
        "  x_test = dataset[\"X_test\"][:]\n",
        "  y_train = dataset[\"y_train\"][:]\n",
        "  y_test = dataset[\"y_test\"][:]\n"
      ],
      "metadata": {
        "id": "5LND81GaiB16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#introduce the channel dimension in the input dataset\n",
        "xtrain = np.ndarray((x_train.shape[0],4096,3))\n",
        "xtest = np.ndarray((x_test.shape[0],4096,3))\n",
        "\n",
        "#iterate in train and test, add the rgb dimension\n",
        "def add_rgb_dimension(array):\n",
        "  scaler_map=cm.ScalarMappable(cmap=\"Oranges\")\n",
        "  array=scaler_map.to_rgba(array)[:,:-1]\n",
        "  return array\n",
        "\n",
        "for i in range(x_train.shape[0]):\n",
        "  xtrain[i] = add_rg_dimension(x_train[i])\n",
        "for i in range(x_test.shape[0]):\n",
        "  xtest[i] = add_rg_dimension(x_test[i])\n",
        "\n",
        "#convert to 1 +4D space (1st argument represents the number of rows in the dataset)\n",
        "xtrain = xtrain.reshape(x_train.shape[0], 16,16,16,3)\n",
        "xtest = xtest.reshape(x_test.shape[0], 16,16,16,3)\n",
        "\n",
        "#convert target variable into one-hot\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n"
      ],
      "metadata": {
        "id": "OcUg_yrgiXfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input layer\n",
        "input_layer = Input((16,16,16,3))\n",
        "\n",
        "#convolutional layers\n",
        "conv_layer1 = Conv3D(filters=8,kernel_size=(3,3,3), activation ='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=16,kernel_size=(3,3,3), activation ='relu')(conv_layer1)\n",
        "\n",
        "#add max pooling to obtain the most informatic features\n",
        "pooling_layer1 = MaxPool3D(pool_size=(2,2,2))(conv_layer2)\n",
        "\n",
        "conv_layer3 = Conv3D(filters=32,kernel_size=(3,3,3), activation ='relu')(pooling_layer1)\n",
        "conv_layer4 = Conv3D(filters=64,kernel_size=(3,3,3), activation ='relu')(conv_layer3)\n",
        "pooling_layer2 = MaxPool3D(pool_size=(2,2,2))(conv_layer4)\n",
        "\n",
        "#perform batch nroalization on the convolution outputs before feeding it to MLP architecture\n",
        "pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
        "flatten_layer = Flatten()(pooling_layer2)\n",
        "\n",
        "#create an MLP architecture with dense layers 4096 > 512 >10\n",
        "#add dropouts to avoid overfitting / perform regularization\n",
        "dense_layer1 = Dense(units=2048, activation = 'relu')(flatten_layer)\n",
        "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
        "dense_layer2 = Dense(units=512, activation = 'relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.4)(dense_layer2)\n",
        "output_layer = Dense(units=10, activation='softmax')(dense_layer2)\n",
        "\n",
        "#define the model with input layer and output layer\n",
        "model = Model(inputs=input_layer, outputs=ouput_layer)"
      ],
      "metadata": {
        "id": "0qAHbW0JkqL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modele.compile(loss=categorical_crossentropy, optimizer=Adadelta(lr=0.1), metrics=['acc'])\n",
        "model.fit(x=xtrain, y=y_train, tach_size=128, epochs=50, validation_split=0.2)"
      ],
      "metadata": {
        "id": "7eE5JVd5nSEC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}